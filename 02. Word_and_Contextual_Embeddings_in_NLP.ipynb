{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word and Contextual Embeddings in Natural Language Processing (NLP)\n",
        "\n",
        "#### After preprocessing and linguistic analysis, the next crucial step in Natural Language Processing is text representation. Machines cannot directly understand words or sentences; therefore, text must be converted into numerical vectors that capture semantic meaning.\n",
        "\n",
        "#### It covers\n",
        "* Static word embeddings (Word2Vec, GloVe)\n",
        "\n",
        "* Contextual word embeddings (BERT, RoBERTa)"
      ],
      "metadata": {
        "id": "pPGD0AiOmOcT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8stzplXUh2f",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf7cc41-1b98-4c19-a74c-1e6bafddd1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Installing requird libraries\n",
        "!pip install nltk spacy gensim transformers torch sentencepiece\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs essential NLP libraries:\n",
        "\n",
        "spaCy – tokenization & linguistic features\n",
        "\n",
        "Gensim – Word2Vec & GloVe\n",
        "\n",
        "Transformers – BERT & RoBERTa models\n",
        "\n",
        "Torch – deep learning backend"
      ],
      "metadata": {
        "id": "05nwswm6mYlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### spaCy provides vector representations for words using pre-trained models. Each word is represented as a high-dimensional numerical vector.\n",
        "\n",
        "##### Key Observation:\n",
        "\n",
        "* Words with similar meanings have similar vector representations\n",
        "\n",
        "* The shape of the vector indicates the dimensionality of the embedding"
      ],
      "metadata": {
        "id": "vYS4Yx5jXEpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\") # Loads spaCy’s English small model\n",
        "text=\"Natural Language Processing is fascinating\"\n",
        "doc=nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text,token.vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOwZOh7hVVPW",
        "outputId": "42d602fa-7521-43ac-d4e6-706b4eb59691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural (96,)\n",
            "Language (96,)\n",
            "Processing (96,)\n",
            "is (96,)\n",
            "fasinating (96,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word2Vec Embeddings (Gensim)\n",
        "\n",
        "##### Word2Vec is a neural network-based technique that learns word embeddings by analyzing word co-occurrence patterns in a corpus.\n",
        "\n",
        "##### It works on the idea that:\n",
        "\n",
        "* Words appearing in similar contexts tend to have similar meanings.\n"
      ],
      "metadata": {
        "id": "f1k07_m8XQEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec using Gensim:\n",
        "from gensim.models import Word2Vec\n",
        "sentances=[\n",
        "    [\"machine\",\"learning\",\"is\",\"fun\"],\n",
        "    [\"natural\",\"languae\",\"processing\"],\n",
        "    [\"deep\",\"learning\",\"models\"]\n",
        "]\n",
        "\n",
        "model=Word2Vec(sentances,vector_size=50,window=3,min_count=1)\n",
        "vector=model.wv[\"learning\"]\n",
        "print(\"Vector for 'learning':\", vector)"
      ],
      "metadata": {
        "id": "YPBvrC6zWp4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c229907-8563-4391-9bed-8dea7c80e91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'learning': [-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
            " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
            " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
            " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
            "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
            "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
            "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
            " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
            "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
            "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
            " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
            " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
            "  9.9641159e-03  1.8466286e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Similarity using Word2Vec\n",
        "\n",
        "#####Word similarity is computed using cosine similarity between vectors. A higher similarity score indicates that words are semantically related.\n",
        "\n",
        "* Note: Training Word2Vec on small datasets is only for demonstration. In practice, large corpora are required."
      ],
      "metadata": {
        "id": "SJWmEL78a7n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word similarity Using word2Vec\n",
        "\n",
        "similarity = model.wv.similarity(\"learning\",\"natural\")\n",
        "print(\"similarity:\",similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaTAGnS-YxR8",
        "outputId": "d355bcb0-f3f6-4747-c431-4341b9115803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity: 0.012442171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GloVe Embeddings (Pre-trained)\n",
        "\n",
        "\n",
        "##### GloVe (Global Vectors) is a word embedding technique that learns representations using global word co-occurrence statistics.\n",
        "\n",
        "##### Unlike Word2Vec:\n",
        "\n",
        "* GloVe embeddings are often used in their pre-trained form\n",
        "\n",
        "* They capture both local and global semantic relationships"
      ],
      "metadata": {
        "id": "9lcVU1LEcSS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe Embedding(pre-trained)\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "glove = api.load(\"glove-wiki-gigaword-50\")\n",
        "print(glove[\"computer\"])\n",
        "\n",
        "print(glove.similarity(\"computer\",\"laptop\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-14kdYiaTrt",
        "outputId": "bc1d1de9-79a6-46c4-98f7-68d287c5f3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "[ 0.079084 -0.81504   1.7901    0.91653   0.10797  -0.55628  -0.84427\n",
            " -1.4951    0.13418   0.63627   0.35146   0.25813  -0.55029   0.51056\n",
            "  0.37409   0.12092  -1.6166    0.83653   0.14202  -0.52348   0.73453\n",
            "  0.12207  -0.49079   0.32533   0.45306  -1.585    -0.63848  -1.0053\n",
            "  0.10454  -0.42984   3.181    -0.62187   0.16819  -1.0139    0.064058\n",
            "  0.57844  -0.4556    0.73783   0.37203  -0.57722   0.66441   0.055129\n",
            "  0.037891  1.3275    0.30991   0.50697   1.2357    0.1274   -0.11434\n",
            "  0.20709 ]\n",
            "0.77411586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Contextual Embeddings using BERT\n",
        "\n",
        "\n",
        "##### Static embeddings assign the same vector to a word regardless of context. However, many words have multiple meanings.\n",
        "\n",
        "Example:\n",
        "\n",
        "* bank (financial institution)\n",
        "\n",
        "* bank (river bank)\n",
        "\n",
        "##### BERT solves this problem by generating different vectors for the same word based on context."
      ],
      "metadata": {
        "id": "3cuxrzBzeqZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# contextual Embeddings using BERT\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model=BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "sentance=\"The Bank is near the river\"\n",
        "inputs=tokenizer(sentance, return_tensors='pt')\n",
        "\n",
        "outputs = model(**inputs)\n",
        "embeddings = outputs.last_hidden_state\n",
        "\n",
        "print(embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmSVZLEEaVQ1",
        "outputId": "3018a1a8-65b2-47ae-8390-32409b58af16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The embeddings generated for the word bank differ depending on the sentence, demonstrating contextual understanding."
      ],
      "metadata": {
        "id": "HKjTXo0hhd2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent1=\"I Deposited money in the bank\"\n",
        "sent2 = \"The river bank is Beautiful\"\n",
        "\n",
        "inputs1 = tokenizer(sent1, return_tensors='pt')\n",
        "inputs2 = tokenizer(sent2, return_tensors='pt')\n",
        "\n",
        "emb1=model(**inputs1).last_hidden_state\n",
        "emb2=model(**inputs2).last_hidden_state\n",
        "\n",
        "print(emb1[0][5][:5])\n",
        "print(emb2[0][5][:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm1a929Icvl4",
        "outputId": "bcde665e-7d30-4f2a-c565-b8e49b88081d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.2485, -0.2934,  0.1040,  0.4932,  0.4474], grad_fn=<SliceBackward0>)\n",
            "tensor([-0.2188, -0.6338, -0.3455,  0.3908,  0.5776], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RoBERTa is an optimized version of BERT with improved training strategies and better performance on many NLP tasks.\n",
        "\n",
        "##### Key Advantages:\n",
        "\n",
        "* Trained on larger datasets\n",
        "\n",
        "* Better handling of language nuances\n",
        "\n",
        "* Stronger contextual representations\n"
      ],
      "metadata": {
        "id": "6PXDSuqwhCRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "model=RobertaModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "inputs=tokenizer(\"Word representaton are powerful \",\n",
        "return_tensors='pt')\n",
        "\n",
        "outputs = model(**inputs)\n",
        "\n",
        "print(outputs.last_hidden_state.shape)\n"
      ],
      "metadata": {
        "id": "mB3s7ylpfpDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7bfb611-0168-4eba-9c5c-8373dd14c360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MeoV-fiGk5pD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}