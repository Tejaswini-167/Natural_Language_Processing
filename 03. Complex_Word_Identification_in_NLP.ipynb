{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Complex_Word_Identification_in_NLP\n"
      ],
      "metadata": {
        "id": "QVrMCIo5hqNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In NLP, Complex Word Identification (CWI) is the task of automatically detecting words that may be difficult for a reader to understand. This task is especially important in applications such as:\n",
        "\n",
        "* Text simplification\n",
        "\n",
        "* Educational tools\n",
        "\n",
        "* Assistive reading systems\n",
        "\n",
        "* Content adaptation for non-native speakers"
      ],
      "metadata": {
        "id": "7xzgQQk4ho3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "two approaches to identify complex word identification:\n",
        "\n",
        "1. A rule-based approach using word length and frequency\n",
        "\n",
        "2. A machine learning-based approach using a Decision Tree classifier"
      ],
      "metadata": {
        "id": "BaUVpFD1h5Ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries and Tools Used\n",
        "\n",
        "**1. NLTK** -  Used to access linguistic corpora such as the Brown Corpus, which provides word frequency statistics.\n",
        "\n",
        "**2. spaCy** - Installed for general NLP support and consistency with previous labs.\n",
        "\n",
        "**3. pandas**  - Used to organize data into tabular form for machine learning.\n",
        "\n",
        "**4. scikit-learn** - Used to build and train a Decision Tree classifier for word complexity prediction."
      ],
      "metadata": {
        "id": "pHGQVTxRiG0B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnfSETaGYdgb",
        "outputId": "84e65519-89fe-419e-a183-6e34979aa990",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk spacy scikit-learn pandas\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Brown Corpus is a standard English corpus containing text from various domains. It is commonly used for linguistic analysis and frequency estimation.\n",
        "### Word Frequency Distribution\n",
        "\n",
        "#### A frequency distribution is created by counting how often each word occurs in the corpus.\n",
        "\n",
        "Why frequency matters:\n",
        "\n",
        "* Common words are generally easier to understand\n",
        "\n",
        "* Rare words are more likely to be complex"
      ],
      "metadata": {
        "id": "_nphDVnliv4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Rule-Based Complexity Detection**\n",
        "\n",
        "#### In the rule-based approach, a word is considered complex if its length exceeds a fixed threshold.\n",
        "\n",
        "Rule Used:\n",
        "\n",
        "If length(word) > 7 → Complex\n",
        "\n",
        "Otherwise → Simple\n",
        "\n",
        "This approach is easy to implement but limited because:\n",
        "\n",
        "* It ignores word meaning\n",
        "\n",
        "* It does not consider context"
      ],
      "metadata": {
        "id": "PZvjS_-8jCaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# frequency based complex word detection\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download(\"brown\")\n",
        "\n",
        "words =brown.words()\n",
        "freq_dist = Counter(word.lower() for word in words)\n",
        "\n",
        "def is_complex(word, threshold=7):\n",
        "    return len(word) > threshold\n",
        "\n",
        "sentence = \"Photosynthesis is an essential biochemical process\".split()\n",
        "\n",
        "for word in sentence:\n",
        "    if is_complex(word):\n",
        "        print(word, \"-> is complex\")\n",
        "    else:\n",
        "        print(word, \"-> is simple\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-x_JutfOaXFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f11963f-f11a-48ca-e934-49f677d7111e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Photosynthesis -> is complex\n",
            "is -> is simple\n",
            "an -> is simple\n",
            "essential -> is complex\n",
            "biochemical -> is complex\n",
            "process -> is simple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Machine Learning-Based Complex Word Identification**\n",
        "\n",
        "\n",
        "\n",
        "#### 1. Feature Extraction\n",
        "\n",
        "Each word is converted into numerical features that help the model decide whether a word is complex or simple.\n",
        "\n",
        "The features used are:\n",
        "\n",
        "Word length – longer words are often more complex\n",
        "\n",
        "Word frequency – rare words are usually harder to understand\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HXYXmV63jJn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### 2. Dataset Preparation\n",
        "\n",
        "A small labeled dataset is created:\n",
        "\n",
        "0 → Simple word\n",
        "\n",
        "1 → Complex word\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rJbWTGakkgTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 3. Model Used: Decision Tree\n",
        "\n",
        "A Decision Tree classifier is used because:\n",
        "\n",
        "* Works well with small datasets and easy to understand\n",
        "\n",
        "* Makes decisions based on clear rule\n",
        "\n"
      ],
      "metadata": {
        "id": "HYegJwdtkYJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 4. Prediction\n",
        "\n",
        "After training, The model predicts whether each word is simple or complex based on learned patterns.\n",
        "\n",
        " --> plant\n",
        "\n",
        " --> metamorphosis\n"
      ],
      "metadata": {
        "id": "LxCkNnZMkYMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def extract_features(word):\n",
        "    return {\n",
        "        \"length\": len(word),\n",
        "        \"frequency\": freq_dist[word.lower()]\n",
        "    }\n",
        "\n",
        "data = [\n",
        "    (\"dog\", 0),\n",
        "    (\"cat\", 0),\n",
        "    (\"photosynthesis\", 1),\n",
        "    (\"biochemical\", 1),\n",
        "    (\"run\", 0),\n",
        "    (\"photosynthetic\", 1)\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"word\", \"label\"])\n",
        "\n",
        "X = pd.DataFrame(df[\"word\"].apply(extract_features).tolist())\n",
        "y = df[\"label\"]\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "test_words = [\"plant\", \"metamorphosis\"]\n",
        "test_features = pd.DataFrame([extract_features(w) for w in test_words])\n",
        "predictions = model.predict(test_features)\n",
        "\n",
        "for word, label in zip(test_words, predictions):\n",
        "    print(word, \"->\", \"complex\" if label else \"simple\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPujs7W7eBtm",
        "outputId": "c3c886e9-1b0f-4fab-ab41-314e1241cfb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "plant -> simple\n",
            "metamorphosis -> complex\n"
          ]
        }
      ]
    }
  ]
}